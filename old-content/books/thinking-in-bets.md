---
title: 'Notes from "Thinking In Bets" by Annie Duke'
author: 'Annie Duke'
authorLast: 'Duke'
bookTitle: 'Thinking In Bets'
date: '2020-01-16'
category: ['reading notes']
rating: 1
tags:
    [
        'rational thought',
        'risk',
        'risk mitigation',
        'optimization',
        'bets',
        'poker',
        'philosophy',
        'decision making',
        'mental model',
        'framework',
    ]
---

Below are the notes I took from Annie Duke's _Thinking In Bets_ in their rawest form. One day maybe I'll go back and refine these. For now, I want the reference since it was a great book!

---

# Introduction: Why This Isn't A Poker Book

Betting makes measuring sticks very easy to compare -> noticed this as she was listing her credentials based on tournament wins and dollars

> winning and losing are only loose signals of decision quality.

> What a bet really is: a decision about an uncertain future.

> The promise of this book is that thinking in bets will improve decision making through our lives. We can get better at separating outcome quality from decision quality, discover the power of saying, "I'm not sure", learn strategies to map out the future, become less reactive decision-makers, build and sustain pods of fellow truth-seekers to improve our decision process, and recruit our past and future selves to make fewer emotional decisions.

-> like this for the clarity that it provides. She listed out what she was going to do with exceptional clarity. Not all of these topics are self-evident in what they mean at this point, but with the benefit of hindsight, it's clear she delivered on her promise.

> Mistakes, emotions, losing — those things are all inevitable because we are human. The approach of thinking in bets moved me _toward_ objectivity, accuracy, and open-mindedness. That movement compounds over time to create significant changes in our lives.

-> It's not binary. The goal is to move forward.

> Thinking in bets starts with recognizing that there are exactly two things that determine how our lives turn out: the quality of our decisions and luck. Learning to recognize the difference between the two is what thinking in bets is all about.

-> It's probably also fair that she'd argue that thinking in bets _helps_ to recognize the difference.

# Chapter 1: Life is Poker, Not Chess

Resulting is the "tendency to equate the quality of a decision with the quality of its outcome.

Exercise idea: what was the best/worst decision of the past year? What about the best/worst outcome?

The focus then, is to tease apart which parts of our actions influenced the outcome and which were beyond our control (i.e. how were we affected by luck)

It's an evolutionary response to downplay the role luck plays ->

> To start, our brains evolved to create certainty and order. We are uncomfortable with the idea that luck plays a significant role in our lives. We recognize the existence of luck, but we resist the idea that, despite our best efforts, things might not work out the way we want. It feels better for us to imagine the world as an orderly place, where randomness does not wreak havoc and things are perfectly predictable. We evolved to see the world that way. Creating order out of chaos has been necessary for our survival.

The "rational" part of the brain can't just pick up more of the load - it's already full.

> "We have this thin layer of prefrontal cortex made just for us, sitting on top of this big animal brain. Getting this thin little layer to handle more is unrealistic. It's already overtaxed."
> — Colin Camerer

-> This speaks to the power of habits and the ability to slow yourself down at the point of decisions - this provides enough time to make a more judicious decision (though it doesn't guarantee that it's right).

> It's hard for a scientist to become a household name. So it shouldn't b surprising that for most people the name John von Neumann doesn't ring a bell.

-> a clean illustration of what some people think of as critical information is totally unknown to others. If you're a game theorist, computer or rocket scientist, it's highly unlikely you don't know von Neumann. But that's not the whole population by any stretch.

> [Jacob] Bronkowski (author of _The Ascent of Man_) quoted von Neumann's response: "'No, no', he said. 'Chess is not a game. Chess is a well-defined form of computation. You may not be able to work out the answers but in theory there must be a solution, a right procedure in any position. Now, real games,' he said, 'are not like that at all. Real life is not like that. Real life consists of bluffing, of little tactics of deception, of asking yourself what is the other man going to think I mean to do. And that is what games are about in my theory.'"

-> reminds me of David Epstein on EconTalk talking about Kind and Unkind learning environments. [[David Epstein on Mastery, Specialization, and Range — EconTalk]]

> Chess, for all its strategic complexity, isn't a great model for decision-making in life, where most of our decisions involve hidden information and a much greater influence of luck.

-> Chess is good for strategy, bad for decision making?

> Making better decisions starts with understanding this: uncertainty can work a lot of mischief.

> Throughly conscious ignorance is the prelude to every real advance in science.
> — James Clerk Maxwell

> "I'm not sure" does not mean that there is no objective truth. It means that we treat our beliefs as works in progress, as under construction.

Before we can acquire _new_ knowledge, we must accept the limits of our current understanding.

When it comes to predicting outcomes, we find that crowds > experts > amateurs. However, since we're focusing on _decision quality_, the ability to arrive and execute on a decision is a distinct advantage of the individual (expert/amateur) over the crowd. The crowds can take advantage of the benefit of large numbers to achieve the prediction advantage.

It's worth remember that increasing odds does _not_ necessarily imply a probable outcome. Sometimes we're making the best of a bad situation.

> When we think in advance about the chances of alternative outcomes and make a decision based on those chances, it doesn't automatically make us wrong when things don't work out. It just means that one event in a set of possible futures occurred.
> Thinking in terms of percentages means theres's a set of possible outcomes. Right and wrong are not the focus.

This does lead me to ask -> what does it mean to be _wrong_? Initially, I was thinking that it would just be when our predictions of probabilities turned out to be wrong. But what about the missing of potential futures? If we didn't even consider a future, that would be a mistake.

> Blaming the oddsmakers or the odds themselves assumes that once something happens, it was bound to have happened and anyone who didn't see it coming was wrong.

> An unwanted result doesn't make our decision wrong if we thought about the alternatives and probabilities in advance and allocated our resources accordingly.

Reminds me of "How Using a Decision Journal can Help you Make Better Decisions"
-> True, however, at some point, what we really care about is the result. So, any decision framework should increase the odds of favorable outcomes.

> Making better decisions stops being about wrong or right but about calibrating among all the shades of grey.

> When the chances are known, we are tethered more tightly to a rational interpretation of the influence of luck.

-> Most of life doesn't allow us to know the odds in advance, however.

> Should we be willing to give up the good feeling of "right" to get rid of the anguish of "wrong"? Yes.
> First the world is a pretty random place…
> Second, being wrong hurts us more than being right feels good.

# Chapter 2: Wanna Bet?

An evaluation of a bet is really an exercise in making opportunity costs explicit.

> [When we make a decision,] we are automatically rejecting every other possible choice.

> When you are betting, you have to back up your belief by putting a price on it.

> [Bets are] a difference of opinion about alternatives, consequences, and probabilities

-> One of the amazing things is how many bets there _are_. If information were perfectly available, it seems like there would be far fewer bets. However, even when we have access to the same information, we interpret it differently. The presence of bets is evidence of this.

> Merriam-Webster's Online Dictionary defines "bet" as "a choice made by thinking about _what will probably happen_," "to _risk_ losing (something) when you try to do or achieve something" and "to make _decisions_ that are based on the _belief_ that something will happen or is true."

-> Bets are:

1. Beliefs -> choices / actions
2. Risk -> Action

Most bets are one-sided bets against ourselves.

> The futures we imagine are merely _possible_.

-> This makes it feel like more information be be sufficient. Schrödinger's cat? Action _reveals_. Foreknowledge is _not_ possible.

> This is ultimately very good news: part of the _skill_ in life comes from learning to be a better belief calibrator.

-> better calibration -> better @ life.

> This is how _we think_ we form abstract beliefs:
>
> 1. We hear something;
> 2. We think about it and vet it, determining whether it is true or false; only after that
> 3. We form our belief.
>    It turns out, though, that we _actually_ form abstract beliefs this way:
> 4. We hear something;
> 5. We believe it to be true;
> 6. Only sometimes, later, if we have the time or the inclination, we think about it and vet it, determining whether it is, in fact, true or false.

> People are credulous creatures who find it very easy to believe and very difficult to doubt. In fact, believing is so easy, and perhaps so inevitable, that it may be more like involuntary comprehension than it is like rational assessment.
> — Daniel Gilbert

> This suggests our default setting is to believe what we hear is true.

-> this is why a microphone is a _biased_ instrument (aka giving someone a microphone via twitter is actually an endorsement of those beliefs).

When we learn things secondhand, we often receive a distilled and processed version of the lesson (e.g., "win big or lose small with suited connectors") we lose valuable context that influenced the initial formation resulting in misunderstanding and misappropriation of the lessons.

> When I taught poker seminars, most of my students strongly believed suited connectors were profitable starting cards under pretty much any circumstances. When I asked why, I would hear "everyone knows that" or "I see players cleaning up with suited connectors all the time on TV." But no one I asked had kept a P&L on their experience with suited connectors.

-> Her brother gave her a menu of cards that she could play, she treated it as a bible. it turned out that those were beginner safe, but because she didn't have the context or all of the information, she would form opinions about other players who played against those diktats. She missed out on learning opportunities because she didn't inquire.

1. We default to belief
2. We then resist change to our belief as we tie them to our identity.

    > We form beliefs without vetting most of them, and maintain them even after receiving clear, corrective information

    -> Hollyn Johnson and Collen Seifert in _Journal of Experimental Psychology_.

> Truthseeking, the desire to know the truth regardless of whether the truth aligns with the beliefs we currently hold, is not naturally supported by the way we process information.

> Hastorf and Cantril concluded, "we do not simply 'react to' a happening… we behave according to what we bring to the occasion." Our beliefs affect how we process all new things, "water the 'thing' is a football game, a presidential candidate, Communism, or spinach."

> Once a belief is lodged, it becomes difficult to dislodge. It takes on a life of its own, leading us to notice and seek out evidence confirming our belief, rarely challenge the validity of confirming evidence, and ignore or work hard to actively discredit information contradicting the belief.
> This is called _motivated reasoning_.

The purpose of fake news is _not_ to convince, but to entrench and amplify already existing beliefs.

> The internet, which gives us access to a diversity of viewpoints with unimaginable ease, in fact speeds our retreat into a confirmatory bubble.

-> one reason i'v tried to use web distribution channels _less_ is that they can be tailored to me. On the other hand, a national newspaper seeking to attract an audience of millions will seek more moderate approaches.

**Lower the cost of being wrong**

> Even when directly confronted with facts that disconfirm our beliefs, we don't let facts get in the way. As Daniel Kahneman pointed out, we just want to think well of ourselves and feel that the narrative of our life story is a positive one. Being _wrong_ doesn't fit into that narrative. If we think of beliefs as only _100% right_ or _100% wrong_, when confronting new information that might contradict our belief, we have only two options: (a) make the massive shift in our opinion of ourselves from 100% right to 100% wrong, or (b) ignore or discredit the new information. It feels bad to be wrong, so we choose (b). Information that disagrees with us is an assault on our self-narrative. We'll work hard to swat that threat away. On the flip side, when additional information agrees with us, we effortless embrace it.

> blind-spot bias — an irrationality where people are better at recognizing biased reasoning in others but are blind to bias in themselves… "furthermore, people who were aware of their own biases were not better able to overcome them."
> (And smarter folks were affected even more).

Data may be unbiased, but we're not and we apply biases to the data as soon as we interpret it.

> When someone challenges us to bet on a belief, singling their confidence that our belief is inaccurate in some way, ideally it triggers us to vet the believe, taking an inventory of the evidence that informed us.

-> When we're challenged, we're presented with an opportunity to revisit ideas that we take for granted.

> And the person who wins bets over the long run is the one with the more accurate beliefs.

-> inherent in this understanding of improving decision making is the willingness to be wrong. You can't be afraid to be wrong or you'll never _take_ the bet.

> "Wanna bet?" Offering a wager, brings the risk out in the open, making explicit what is already implicit (and frequently overlooked). The more we recognize that we are betting on our beliefs (with our happiness, attention, health, money, time, or some other limited resource), the more we are likely to temper our statements, getting closer to the truth as we acknowledge the risk inherent in what we believe.

> There is always a degree of uncertainty… practically nothing is black and white, 0% or 100%. And that's a pretty good philosophy for living.

"The expiration date of facts."

> Given that even scientific facts can have an expiration date, we would all be well-advised to take a good hard look at our beliefs.

Not _if_ (which is a yes/no), but _how much_ (a spectrum).

> Incorporating uncertainty in the way we think about what we believe creates open-mindedness,. Moving us closer to a more objective stance toward information that disagrees with us.

> our narrative fo being a knowledgeable, educated, intelligent person who holds quality opinions isn't compromised when we use new information to calibrate our beliefs, compared with having to make a full-on reversal.

-> Calibration vs. reversals.

Which in turn…

> makes us hungry for information to fill in the gaps of our knowledge. It makes us grateful for opposing viewpoints. Instead of looking for ways to confirm we're right, we're more likely to ask, "Why am I wrong?"

> there is no sin in finding out there is evidence that contradicts what we believe. The only sin is in not using that evidence as objectively as possible to refine that belief going forward.

Acknowledging uncertainty less credibility to our position and us as communicators. .

Uncertainty opens the door and welcomes other perspectives to give us new information.

> Statements of science are not of what is true and what is not true, but statements of what is known to different degrees of certainty… Every on of the concepts of science is on a scale graduated somewhere between, but nat neither end of, absolute falsity or absolute truth.

-- Richard Feynman

# Chapter 3: Bet to Learn: Fielding the Unfolding Future

> While experience is necessary to becoming an expert, it's not sufficient.
> Experience can be an effective teacher. But, clearly, only some students listen to their teachers.

-> When it comes to learning about how to be an engineer, husband, son, father, brother, etc., there's no point in getting frustrated that I don't know everything, all I can do is seek out opportunities to increase my experiences.

> Experience is not what happens to a man; it is what a man does with what happens to him.
> — Aldous Huxley
> The active process of gaining experience

> The more evidence we get from experience, the less uncertainty we have about our beliefs and choices.

-> one reason why experts are confident. Also why age tends to make folks more confident - they've seen more.

> Chalk up an outcome to skill, and we take credit for the result. Chalk up an outcome to luck, and it wasn't in our control.

-> Figuring out which is what we mean by "fielding".

We learn things from luck, but mostly that its' not worth updating our beliefs based on the results.

> outcomes don't tell us what's our fault and what isn't, what we should take credit for and what we shouldn't.

We won/ we're great <—> We lost / we had bad luck
Consistently only considering these options is a "fielding error" - it's also referred to as a "self-serving bias"

> Classical stimulus-response experiments have shown that the introduction of uncertainty drastically slows learning.
> (See the kind/unkind learning environments again).

We often forget that the absence of a bad outcome can also indicate the presence of luck… we just don't consider it.

> Because we're good at detecting deception, to deceive others about our self-confidence, we had to first deceive ourselves.

> When our self-engage is at stake, we treat our fielding decisions as 100% or 0%: right versus wrong, skill versus luck, our responsibility versus outside our control. There are no shades of grey.

> This is a systematic bias… there are some people, to be sure, who exhibit the opposite of self-serving bias, treating everything bad that happens as their fault and attributing anything good in their lives to luck… if we can't find a way to value _accuracy_ in fielding outcomes, we are going to throw away a lot of learning opportunities regardless of which kind of error we make.
> (Emphasis mine).

> We must believe in luck. For how else can we explain the success of those we don't like?
> — Jean Cocteau

> The systematic errors in the way we field the outcomes of our peers comes at a real cost. It doesn't just come at the cost of reaching our goals but also at the cost of compassion for others.

-> when we fail to try to understand _why_ people behave the way they do, we lose compassion and lay blame at their feet.

> Schadenfreude is basically the opposite of compassion.

> What accounts for most of variance in happiness is how we're doing _comparatively_.

-> Objective measures of happiness contribute only a small fraction - majority comes from how we perceive we're doing relative to others.

Changing habits can be thought of "changing the bell"

> The people with the most legitimate claim to a bulletproofself-narrative have developed habits around self-critique.

-> Claim: top performers are less likely to allow self-serving bias interference with their development -> trying to think about this w/r/t LBJ who suppressed his ego in subservience to his ambition.

> Many people say I'm the best women's soccer player in the world. I don't think so. An because of that, someday I just might be.
> — Mia Hamm.

> Keep the reward of feeling like we are doing well compared to our peers, but change the features by which we compare ourselves: be a better credit-giver than your peers, more willing than others to admit mistakes, more willing to explore possible reasons for an outcome with an open mind, even, and especially, if that might cast you in a bad light or shine a good light on someone else.

-> We _can_ be exceptional, but one way way is it to redefine the plane of competition. Feels a bit like a cheat code.

> Identifying learning opportunities that other players were missing made me feel good about myself, reinforcing my routine change.

-> when you start to focus on the improving the process, not on defining yourself by the outcomes, you'll get feedback that what you're doing ins hard — which will feel good.

> The key is that in explicitly recognizing that the way we field an outcome is a bet, we consider a greater number of alternative causes more seriously than we otherwise would have.

-> Recognize that there is rarely just _one_ plausible explanation for any outcome.

> Perspective taking gets us closer to the truth because that truth generally lies in the middle of the way we field outcomes for ourselves and the way we field them for others.

-> remember, we're great at identifying fault in others, but terribly blind in ourselves — this modulates that experience by helping us take _both_ perspectives.

> Treating outcome fielding as bets constantly reminds us outcomes are rarely attributable to a single cause and there is almost always uncertainty in figuring out the various causes.

-> Because of this, we can be more compassionate.

The refinements in our understanding is a calibration method. It avoids bad outcomes from compounding. It serves as its own compounding benefit - the more we do, the more opportunities we see, and the bigger the base we have to make incremental gains from.

# Chapter 4: The Buddy System

> Let me give you an example from my own life… for a long time… I thought, 'Geeze, people are idiots.' Then it occurred to me, 'Is it possible that everybody's an idiot? Maybe I'm the idiot,' and it turns out I am.
> — David Letterman
> When circumstances repeat themselves and the only common link is us, it's possible / probable we're the cause.

When people tell stories, it's worth asking what they are looking for (advice or sympathy) _before_ weighing in.

> If I wanted to engage that group about poker, I had to ask about my strategic decisions… because I agreed to the group's rules of engagement.

-> Each group / friendship likely has norms of engagement - understanding what they are can help to ensure that yo udon't violate them.

Truthseeking - two distinct approaches that are context dependent

1.  W/in decision pod can be direct
2.  Outside the decision pod be more gentle.

> A long as there are three people in the group (two to disagree and one to referee), the truth seeking group can be stable and productive

> Whereas confirmatory thought involves a one-sided attempt to rationalize a particular point of view, exploratory thought involves even handed consideration of alternative points of view."
> — Philip Tetlock and Jennifer Lerner

-> really like this idea of exploratory vs. confirmatory thought.

Group's interest in accuracy and diversity of thought -> improved decisions of individuals and increased accountability -> not only do you know you'll be accountable, but the group can work with you to further improve your decisions in the future, even better, you are the beneficiary of everyone _else's_ experiences that you get to learn from .

A "blueprint" for a truth seeking charter:

> 1. Focus on accuracy (over confirmation), which includes rewarding truth seeking, objectivity, and open-mindedness within the group;
> 2. Accountability, for which members have advance notice; and
> 3. Openness to a diversity of ideas.

> In the long run, the more objective person will win against the more biased person. In that way, betting is a form of accountability to accuracy.

-> in order to get there, you need to look honestly and earnestly at the facts — hiding / ignoring facts will hinder you.

> Accountability made me run that conversation in my head

-> knowing you will have to explain yourself can constrain actions -> it's when there are not consequence that we do stupid stuff.

> It's when there are not consequences that we do stupid things. Whether that's because we're not held accountable by friends or the law.

> To get a more objective view of the world, we need an environment that exposes us to alternate hypotheses and different perspectives. That doesn't apply only to the world around us: to view _ourselves_ in a more realistic way, we need other people to fill in our blind spots.

pg. 139 - This almost feels like a better defense of reading since that allows you to inhabit another's mind - the benefit of people is that they can respond to your specific situation.

> Others aren't wrapped up in reserving our narrative, anchored by our biases. It's a lot easier to have someone else offer their perspective than for you to imagine you're another person and think about what their perspective might be.

-> One reason why it's almost always easier to _give_ advice to someone else than yourself.

> Except for whatever concern another hods for us, they have no interest in allowing / preserving a false image of who we are.

> Peer review, the gold standard that epitomizes the open-mindedness and hypothesis testing of the scientific method, "offers much less protection against error when the community of peers is politically homogenous." (Citing a paper by the Heterodox Academy)

> People are more willing to offer their opinion when the goal is to win a bet rather than get along with people in a room.

-> bets also create a separation between an idea's champion and the expected result (i.e. divorces the person from the idea).

# Chapter 5: Dissent To Win

CUDOS -

> **C**ommunism (data belong to the gorup),
> **U**niversalims (apply uniform standards to claims and evidence, regardless of where they came from),
> **D**isinterestedness (vigilance against potential conflicts that can influence the groups evaluation), and
> **O**rganized **S**kepticism (discussion among the group to encourage engagement and dissent).

> Secrecy is the antithesis of this norm [communism]; full and open communication its enactment.
> — Robert K. Merton

Note, however, that the norm is not transparency always and without boundaries, but _within_ the group -> this then, creates a different problem of defining us and them, drawing the bounds of the boundary.

Sharing, and the subsequent exchange of ideas can redefine our interpretation of facts by understanding more perspectives.

> We are naturally reluctant to share information that could encourage others to find fault in our decision-making. My group made this easier by making me feel good about committing myself to improvement.

-> The groups' power / opportunity to reward vulnerability (action), not being wrong or right (result).

> Don't disparage or ignore an idea just because you don't like who or where it came from.

-> is it reasonable to believe that the originator of an idea has no influence on its formation? No, but the point is that the idea, claim, etc. has merit separate from its origin — a distinct vector for analysis.

> The substance of information has merit (or lack of merit) separate from where it came from.

> When I had the impulse to dismiss someone as a bad player, I made myself find something that they did well.
> […]
> I had started thinking more deeply about the way my opponents thought.
> […]
> And my poker group benefitted from this exercise as well, because in workshopping the strategies with each other, we multiple the number of playing techniques we could observe and discuss.

-> this is basically just an endorsement of cooperation. By working with others, Annie was able to get more done than if she'd been left to her own devices. Still, it's an effective anecdote.

> Also, by refusing to dismiss an opponent 'out of hand' you find that you must inhabit their mind more deeply to understand their motivations.

[musing] What would happen if we, in exchange for spectrum required a broadcaster to have another spectrum owner broadcast on their channel for 10 minutes ever hour? Would they pick extreme topics? Or moderate ones? e.g., CNBC and Fox had a programming exchange.

> So leave the source out to start, giving the group the maximum opportunity to form an impression without shooting (or celebrating) the message based on their opinion of the messenger (separate from the expertise and credibility fo the messenger).

-> When retelling a story or seeking advice, actively separate the message from the messenger.

> He [Feynman] found that if those analyzing data knew, or could even just intuit, the hypothesis being tested, the analysis would be more likely to support the hypothesis being tested. The measurements might be objective, but slicing and dicing the data is vulnerable to bias, even unconsciously.

-> Interpretation supports the hypothesis — which is interesting because the analysis was motivated by having a hypothesis in the first place.

> The best way to do this is to deconstruct decisions before an outcome is known. […] After the outcome, make it a habit when seeking advice to give the details without revealing the outcome.

Coupled with the "leave out the source at start" - we're not really breaking down stories into three component parts: the message, the messenger, and the outcome.

> Simply put, the group is less likely to succumb to ideological conflicts of interest when they don't know what the interest is.

-> Burden is on the presenter to _not_ share the outcome or beliefs of how it _should_ have proceeded.

Rules for "Communicating with the world beyond our group"

1. Express uncertainty
2. Lead with assent
3. Ask for a temporary agreement to engage in truth-seeking
4. Focus on the future.

> If we want to engage someone with whom we have some disagreement (inside or outside our group), they will be more open and less defensive if we start with those areas agreement, which there surely will be.

> When the new information is presented as _supplementing_ rather than _negating_ what has come before, our listeners will be much more open to what we have to say.

> "And" is an offer to contribute. "But" is a denial and repudiation fo what came before.

# Chapter 6: Adventures in Mental Time Travel

> Accountability to our group can pop us briefly int the future to imagine the conversation about the decision we will have with the group. Running that conversation will often remind us to stay on a more rational path.
> The accountability creates consequences. This is an example of how ew can leverage consequences to _serve us_.
> Context collapse occurs when we refuse to look at our past and future selves.

On regret

> [Remorse was] adding to the first act of stupidity a second.
> — Friedrich Nietzsche
> Make the most of your regrets; never smother your sorrow, but tend and cherish it till it comes to have a separate and integral interest. To regret deeply is to live afresh.
> — Henry David Thoreau

> The problem isn't so much whether regret is an unproductive emotion. It's that regret occurs after the fact, instead of before.

Use regret-in-advance as an input into decisions. Regret-post-hoc should be discarded

> Every 10-10-10 process starts with a question… [W]hat are the consequences of each of my options in ten minutes? In ten months? In ten years?
> — Suzy Welch

> Coming to peace with a bad outcome in advance will feel better than refusing to acknowledge it, facing it only after it has happened.
> [[On Finishing Well]]

> Imagine your are standing on a narrow strip of concrete on the shoulder of the highway. Behind you is your car, hazard lights flashing. The rear tire on the driver's side is shredded. It is fully dark and the drizzle has turned into a cold, heavy downpour. You've called roadside assistance, twice, and both times (after long hold times) spoken with operators who've told you someone will arrive "as soon as they get there after responding to your call." You decide to change the tire yourself, only to discover you have no jack. You're soaked to the skin and cold.
> How does it feel? It likely feels like the worst moment of your life. You are likely bemoaning how unlucky you are, wondering why these things always happen to you. You are miserable and you can't imagine feeling any other way.
> That's how it feels in the moment. But if the flat tired had happened a year ago, do you think it would have an effect on your happiness today, or your overall happiness over the past year? Not likely. It likely wouldn't cause your overall happiness to tick up or down. It would probably have faded to a funny story (or a story you try to make sound funny) told at cocktail parties.

-> equanimity, "this too shall pass", "we will see" [[Perhaps]]

Deviations are much smaller if viewed from a long-term perspective. Day-to-day changes rarely have a lasting impact. <- this is a nice reminder

The stress of constant feedback can distort decision quality and our response to outcomes. -> Finding ways to "trick" ourselves into only viewing things from a general trend can ease stress.

> For people involved in specialized activities, it's worth it to be able to communicate a complex concept in a single word that laypeople would need lengthy phrases to convey

-> a defense of jargon and its power to tie communities together -> you feel like you're in if you understand what's being said.

Tilt: When we allow our emotional state to be affected by the outcome -> which will then affect the future decisions (or the possibility of it at a minimum)

Ulysses Contracts -> Literally binding our hands in advance of a future decision.

> Ulysses contracts can come in varying levels of how much your hands are bound, ranging from physically preventing acting on a decision or just committing in advance to certain actions without barriers save the commitment itself. Regardless of the level of binding, precommitment contracts trigger a decision-interrupt. At the moment when we consider breaking the contract, when we want to cut the binding, we are much more likely to stop and think.
> When you are physically prohibited from deciding, you are interrupted in the sense that you are prevented from acting on an irrational impulse; the option simply isn't there. That's the brute-force way to do this kind of time traveling. Past-Ulysses interrupted present-Ulysses's decision by taking the decision, literally, out of his hands.

Decision Swear Jar and potential violations to watch out for:

1. Signs of the illusion of certainty
2. overconfidence
3. Irrational outcome fielding
4. Any kind of moaning or complaining about bad luck just to off-load it, with no real point other than to get sympathy
5. Generalized characterizations of people mean tot dismiss their ideas
6. Other violations of the Mertonian norm of universalism (e.g., shooting the messenger)
7. Signals that we have zoomed in on a moment, out of proportion with the scope of time.
8. Expressions that explicitly signal motivated reasoning, accepting / rejecting information without much evidence.
9. The word "wrong"
10. Lack of self-compassion
11. Signals we're being overly generous editors when we share a story
12. Infecting our listeners with a conflict of interested - including our own conclusions
13. Terms that discourage engagement of others

Scenario planning - not about which future _will_ occurs (a conclusion), but the probability of many.

> People frequently resist having to make a guess at the probability of future events mainly because they feel like they can't be certain of what the likelihood of any scenario is. But that's the point.
> The reason why we do reconnaissance is _because_ we are uncertain. We don't (and likely can't) know how often things will turn out a certain way twitch exact precision. It's not about approaching our future predictions from a point of perfection. It's about acknowledging that we're already making a prediction about the future every time we make a decision, so we're better off if we make that explicit.

> The best players think beyond the current hand into subsequent hands: how do the actions of this hand affect how they and their opponents make decisions on future hands?

> Being able to respond to the changing future is a good thing; being surprised by the changing future is not.

-> If you don't spend Tim shrinking about what the future _will_ look like, you're more likely to be surprised by what unfolds.

> By mapping out the potential futures and probabilities, we are less likely to fall prey to resulting or hindsight bias.

Good decisions, poorly articulated, can erode trust -> this can be more damaging than a bad decision to the cohesion of a team. -> this is in response to Annie's statement:

> I'd like to think that Pete Carroll didn't lose much sleep over his decision to call for Wilson to pass.
> I say that because even if Carroll's decision is the best one eh could have made (and she makes a pretty good argument for its defense), if his _team_ doesn't believe that, it matters _way less_.

> When it comes to advance thinking, standing at the end and looking backward is much more effective than looking forward from the beginning.

-> Backcasting (or "prospective hindsight") is the exercise of jumping into the future and then trying to imagine which actions you took to arrive at a particular future.

> Implicit in that approach [of envisioning the future from the present] is the assumption that conditions will remain the same, facts won't change, and the paradigm will remain stable. The world changes to o fast to assume that approach is generally valid.

Premortems: working backward from a negative future
-> this is a solution to the fact that backcasting is focused on one possible future outcome only. By now identifying reasons why that future _failed_ to materialize you acknowledge other futures.

> Oettingen recognized that we need to have positive goals, but we are more likely to execute on those goals if we think about the negative futures. We start a premortem by imagining why we failed to reach our goal. […] Then we imagine why. All those reasons why we didn't achieve our goal help us anticipate potential obstacles and improve our likelihood of succeeding.

> [In a premortem] People can express their reservations without it sounding like they're saying the planned course of action is wrong. Because of that, a planning process that includes a premortem creates a much healthier organization because it means that the people who do have dissenting opinions are represented in the planning. They don't feel like they're shut out or not being heard. Everyone's voice now has more value. The organization is less likely to discourage dissent and thereby lose the value of diverse opinions. Those who have reservations are less likely to have resentment or regret build if things don't work out; their voices were represented in the strategic plan.

> Remember, the likelihood of positive and negative futures must add up to 100%.
> If it doesn't, you're a. estimating poorly or b. neglecting potential futures.

> One fo the goals of mental time travel is keeping events in perspective. To understand an overriding risk to that perspective, think about time as a tree. The tree has a trunk, branches at the top, ad the place where the trunk meets the branches. The trunk is the past. A tree has only one, growing trunk, just as we have only one, accumulating past. The branches are the potential futures. Thicker branches are the equivalent of more probable futures, thinner branches are less probable ones. The place where the top of the trunk meets the branches is the present. There are many futures, many branches of the tree, but only one past, one trunk.

-> there are details to quibble with here, but I like the notion of thinking of time, past, present, future as a tree.

> Once something occurs, we no longer think fo it as probabilistic—or as \_ever- having been probabilistic.

> And even when we make a bad bet, we usually get a second chance because we can learn from the experience make a better bet the next time.
> Life, like poker, is one long game, and there are going to be a lot of losses, even after making the best possible bets.
